---
title: 'Self-Attention for Incomplete Utterance Rewriting'
authors:
  - Yong Zhang
  - Zhitao Li
  - Jianzong Wang
  - Ning Cheng
  - Jing Xiao
corresponding_author:
    - ''
    - ''
    - 'Corresponding author'
date: '2022-05-23T00:00:00Z'
doi: ''


# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ['1']

# Publication name and optional abbreviated publication name.
publication: In *2022 IEEE International Conference on Acoustics, Speech and Signal Processing*
publication_short: In *ICASSP2022* (CCF-B)

abstract: Incomplete utterance rewriting (IUR) has recently become an essential task in NLP, aiming to complement the incomplete utterance with sufficient context information for comprehension. In this paper, we propose a novel method by directly extracting the coreference and omission relationship from the self-attention weight matrix of the transformer in-stead of word embeddings and edit the original text accordingly to generate the complete utterance. Benefiting from the rich information in the self-attention weight matrix, our method achieved competitive results on public IUR datasets.


tags:
  - Other
featured: true
links:
- name: "arXiv"
  url: 'https://arxiv.org/abs/2202.12160'
- name: "IEEE"
  url: 'https://ieeexplore.ieee.org/document/9746341'
url_pdf: ''
url_code: ''
url_poster: ''
url_slides: ''

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: 'The architecture of our proposed model'
  focal_point: ''
  preview_only: false


---

{{% callout note %}}
Click the _Cite_ button above to demo the feature to enable visitors to import publication metadata into their reference management software.
{{% /callout %}}

